{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L99-tuLdRiVJ"
   },
   "source": [
    "##### На последнем семинаре мы проанализировали несколько различных морфологических теггеров. Как же узнать, какой использовать? Давайте сравним их качество!\n",
    "##### В этой домашке вам будет нужно найти или самим написать русский и английский тексты (каждый от ста слов), в которых  будут какие-то трудные или неоднозначные для POS теггинга моменты и разметить их в ручную (а потом объяснить, какие моменты вы тут считаете трудными для автоматического посттеггинга и почему) – с помощью этих текстов мы будем оценивать качество работы наших теггоров. В текстах размечаем только части речи, ничего больше!\n",
    "##### Вы получаете балл за создание, разметку текста и объяснение того, почему этот текст подходит для оценки (что в нём сложного). Всего за этот пункт 2 балла, т. к. языка 2.\n",
    "##### Потом вам будет нужно взять три  POS теггера для русского (pymorphy2, mysteam, Natasha) и 3  - для английского (SpyCy, Flair, NLTK) и «прогнать» текст через каждый из них (если вы запустите только 2 теггера из трёх – получите балл, если три  из трёх – 2 балла, т. е. суммарно за этот пункт можно получить 4 балла).\n",
    "##### После этого вам надо будет оценить accuracy для каждого теггера. Заметьте, что в разных системах имена теггов и части речи  могут отличаться, – вам надо будет свести это всё к единому стандарту с помощью какой-то функции или кода и сравнить с вашим размеченным руками эталоном - тоже с помощью какого-то кода или функции. Этот пункт стоит 2 балла.\n",
    "##### Тут вы уже получили 8 баллов.\n",
    "##### Дальше вам нужно взять лучший теггер для русского языка и  с его помощью написать функцию, которая повысит качество работы программы из первой домашки. Так, многие из вас справедливо заметили, что если бы мы могли класть в словарь не только отдельные слова, но и словосочетания, то программа работала бы лучше. Вам надо выделить 3 вида синтаксических групп (к примеру не + какая-то часть речи или NP или сущ.+ наречие или еще что-то), запись которых в словарь, по вашему мнению, улучшила бы качество работы программы и создать такую функцию или функции, которые с помощью любых известных нам средств (chunking и regexp grammar, Natasha syntax parser,  код с последнего занятия по SpyCy, etc.) будет выделять эти группы в поданном в нее тексте. Два балла за саму функцию, балл за объяснение того, почему именно эти группы вы взяли.\n",
    "##### 2 бонусных балла, если встроите эту функцию в программу из предыдущей домашки и сравните качества работы программы с нею и без неё.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Аня Смирнова БКЛ181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "работа с текстом на русском языке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jU9_6DdIRiVM"
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uCxy40MXRiVT"
   },
   "outputs": [],
   "source": [
    "text = 'Закусочная, в которой остановился Вишневский, находилась в десяти милях от Нью-Джерси. '+\\\n",
    "'За соседним столом сидел полицейский с девушкой, которая пила апельсиновый сок. '+\\\n",
    "'На стене висела железная пила с красной деревянной ручкой. В течение долгого времени кругом царила тишина. '+\\\n",
    "'Затем зашла женщина лет тридцати, выложила на стол журнал с надписью \"АМН\", как оказалось позже, что она ученый, '+\\\n",
    "'а ее отец - военный. Солнце за окном казалось большим огненным кругом, который вот-вот взорвется. '+\\\n",
    "'Женщина спела песню, ее голос был звонок. Вдруг я осознала, что я потеряла мою записную книжку, которую я очень берегу. '+\\\n",
    "'В тот же момент раздался звонок от моего знакомого. Он заехал ко мне домой, приготовил жаркое и убрался в гостинной. '+\\\n",
    "'Он привез вишню, она была спела и сладка. Затем он взял такси и отправился к берегу реки. '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Vm5jZmHRiVb"
   },
   "source": [
    "в чем тут сложность: тут есть слова-омонимы (пила,кругом,спела,звонок,мою), имена собственные, похожие на прил (Вишневский), существительные, похожие на прилагательные (военный, знакомый, гостинная, жаркое, закусочная), аббревиатура (АМН), слова через дефис (вот-вот) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnAbuFEnRiVc"
   },
   "source": [
    "разбор при помощи mystem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "id": "UpvJ7qU-RiVd",
    "outputId": "c80f360f-0306-4cf9-ac13-4e5950d850f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOUN', 'P', 'PRON', 'VERB', 'NOUN', 'VERB', 'P', 'NUM', 'NOUN', 'P', 'NOUN', 'P', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'P', 'NOUN', 'PRON', 'VERB', 'ADJ', 'NOUN', 'P', 'NOUN', 'VERB', 'ADJ', 'NOUN', 'P', 'ADJ', 'ADJ', 'NOUN', 'P', 'NOUN', 'ADJ', 'NOUN', 'ADV', 'VERB', 'NOUN', 'ADV', 'VERB', 'NOUN', 'NOUN', 'NUM', 'VERB', 'P', 'NOUN', 'NOUN', 'P', 'NOUN', 'NOUN', 'CONJ', 'VERB', 'ADV', 'CONJ', 'PRON', 'NOUN', 'CONJ', 'PRON', 'NOUN', 'NOUN', 'NOUN', 'P', 'NOUN', 'VERB', 'ADJ', 'ADJ', 'NOUN', 'PRON', 'ADV', 'VERB', 'NOUN', 'VERB', 'NOUN', 'PRON', 'NOUN', 'VERB', 'NOUN', 'ADV', 'PRON', 'VERB', 'CONJ', 'PRON', 'VERB', 'PRON', 'ADJ', 'NOUN', 'PRON', 'PRON', 'ADV', 'NOUN', 'P', 'PRON', 'PART', 'NOUN', 'VERB', 'NOUN', 'P', 'PRON', 'NOUN', 'PRON', 'VERB', 'P', 'PRON', 'ADV', 'VERB', 'ADJ', 'CONJ', 'VERB', 'P', 'ADJ', 'PRON', 'VERB', 'NOUN', 'PRON', 'VERB', 'VERB', 'CONJ', 'ADJ', 'ADV', 'PRON', 'VERB', 'NOUN', 'CONJ', 'VERB', 'P', 'NOUN', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "mystemtags = []\n",
    "ana = m.analyze(text)\n",
    "for word in ana:\n",
    "    if 'analysis' in word:\n",
    "        gr = word['analysis'][0]['gr']\n",
    "        pos = gr.split('=')[0].split(',')[0]\n",
    "        if pos == 'S':\n",
    "            pos = pos.replace(pos, 'NOUN')\n",
    "        if pos == 'PR':\n",
    "            pos = pos.replace(pos, 'P')\n",
    "        if pos == 'V':\n",
    "            pos = pos.replace(pos, 'VERB')\n",
    "        if pos == 'A':\n",
    "            pos = pos.replace(pos, 'ADJ')\n",
    "        if pos == 'SPRO' or pos == 'APRO':\n",
    "            pos = pos.replace(pos, 'PRON')\n",
    "        mystemtags.append(pos)\n",
    "print(mystemtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDvEhpGMRiVl"
   },
   "source": [
    "разбор при помощи pymorphy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "q86uTZCoRiVm",
    "outputId": "26759399-76cd-4563-8fd2-285d6c363b14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Закусочная', 'в', 'которой', 'остановился', 'Вишневский', 'находилась', 'в', 'десяти', 'милях', 'от', 'Нью-Джерси', 'За', 'соседним', 'столом', 'сидел', 'полицейский', 'с', 'девушкой', 'которая', 'пила', 'апельсиновый', 'сок', 'На', 'стене', 'висела', 'железная', 'пила', 'с', 'красной', 'деревянной', 'ручкой', 'В', 'течение', 'долгого', 'времени', 'кругом', 'царила', 'тишина', 'Затем', 'зашла', 'женщина', 'лет', 'тридцати', 'выложила', 'на', 'стол', 'журнал', 'с', 'надписью', 'АМН', 'как', 'оказалось', 'позже', 'что', 'она', 'ученый', 'а', 'ее', 'отец', 'военный', 'Солнце', 'за', 'окном', 'казалось', 'большим', 'огненным', 'кругом', 'который', 'вот-вот', 'взорвется', 'Женщина', 'спела', 'песню', 'ее', 'голос', 'был', 'звонок', 'Вдруг', 'я', 'осознала', 'что', 'я', 'потеряла', 'мою', 'записную', 'книжку', 'которую', 'я', 'очень', 'берегу', 'В', 'тот', 'же', 'момент', 'раздался', 'звонок', 'от', 'моего', 'знакомого', 'Он', 'заехал', 'ко', 'мне', 'домой', 'приготовил', 'жаркое', 'и', 'убрался', 'в', 'гостинной', 'Он', 'привез', 'вишню', 'она', 'была', 'спела', 'и', 'сладка', 'Затем', 'он', 'взял', 'такси', 'и', 'отправился', 'к', 'берегу', 'реки']\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "text1 = [word.strip(punctuation) for word in text.split()]\n",
    "text1 = [word for word in text1 if word != '']\n",
    "print(text1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Z2Wfo9lLRiVt",
    "outputId": "9e6b2473-de35-42c9-8e3d-055aba8e59ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADJ', 'P', 'ADJ', 'VERB', 'NOUN', 'VERB', 'P', 'NUM', 'NOUN', 'P', 'NOUN', 'P', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'P', 'NOUN', 'ADJ', 'NOUN', 'ADJ', 'NOUN', 'P', 'NOUN', 'VERB', 'ADJ', 'NOUN', 'P', 'ADJ', 'ADJ', 'NOUN', 'P', 'NOUN', 'ADJ', 'NOUN', 'ADV', 'VERB', 'NOUN', 'ADV', 'VERB', 'NOUN', 'NOUN', 'NUM', 'VERB', 'P', 'NOUN', 'NOUN', 'P', 'NOUN', 'NOUN', 'CONJ', 'VERB', 'COMP', 'CONJ', 'PRON', 'ADJ', 'CONJ', 'PRON', 'NOUN', 'NOUN', 'NOUN', 'P', 'NOUN', 'CONJ', 'ADJ', 'ADJ', 'ADV', 'ADJ', 'ADV', 'VERB', 'NOUN', 'VERB', 'NOUN', 'PRON', 'NOUN', 'VERB', 'NOUN', 'ADV', 'PRON', 'VERB', 'CONJ', 'PRON', 'VERB', 'ADJ', 'ADJ', 'NOUN', 'ADJ', 'PRON', 'ADV', 'NOUN', 'P', 'ADJ', 'PART', 'NOUN', 'VERB', 'NOUN', 'P', 'ADJ', 'ADJ', 'PRON', 'VERB', 'P', 'PRON', 'ADV', 'VERB', 'NOUN', 'CONJ', 'VERB', 'P', 'ADJ', 'PRON', 'VERB', 'NOUN', 'PRON', 'VERB', 'VERB', 'CONJ', 'ADJS', 'ADV', 'PRON', 'VERB', 'NOUN', 'CONJ', 'VERB', 'P', 'NOUN', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "pymorphytags = []\n",
    "for word in text1:\n",
    "    word = morph.parse(word)\n",
    "    w = word[0]\n",
    "    tag = w.tag.POS\n",
    "    if tag == 'PREP':\n",
    "        tag = tag.replace(tag,'P')\n",
    "    if tag == 'ADJF':\n",
    "        tag = tag.replace(tag,'ADJ')\n",
    "    if tag == 'NUMR':\n",
    "        tag = tag.replace(tag,'NUM')\n",
    "    if tag == 'NPRO':\n",
    "        tag = tag.replace(tag,'PRON')\n",
    "    if tag == 'ADVB':\n",
    "        tag = tag.replace(tag,'ADV')\n",
    "    if tag == 'PRCL':\n",
    "        tag = tag.replace(tag,'PART')\n",
    "    pymorphytags.append(tag)\n",
    "print(pymorphytags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "разбор при помощи natasha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wDbgcTPlRiVz"
   },
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    \n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "\n",
    "    Doc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "eBsfjg8PRiV-"
   },
   "outputs": [],
   "source": [
    "segmenter = Segmenter()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IWgfJP_BRiWF"
   },
   "outputs": [],
   "source": [
    "doc = Doc(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "J0CznJWURiWL"
   },
   "outputs": [],
   "source": [
    "doc.segment(segmenter)\n",
    "doc.tag_morph(morph_tagger)\n",
    "doc.parse_syntax(syntax_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "TREuqhxxRiWR",
    "outputId": "1e25021e-4ee6-4804-969c-c702757681c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOUN', 'P', 'PRON', 'VERB', 'NOUN', 'VERB', 'P', 'NUM', 'NOUN', 'P', 'NOUN', 'P', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'P', 'NOUN', 'PRON', 'VERB', 'ADJ', 'NOUN', 'P', 'NOUN', 'VERB', 'ADJ', 'NOUN', 'P', 'ADJ', 'ADJ', 'NOUN', 'P', 'NOUN', 'ADJ', 'NOUN', 'ADV', 'VERB', 'NOUN', 'ADV', 'VERB', 'NOUN', 'NOUN', 'NUM', 'VERB', 'P', 'NOUN', 'NOUN', 'P', 'NOUN', 'NOUN', 'CONJ', 'VERB', 'ADV', 'CONJ', 'PRON', 'NOUN', 'CONJ', 'PRON', 'NOUN', 'ADJ', 'NOUN', 'P', 'NOUN', 'VERB', 'ADJ', 'ADJ', 'NOUN', 'PRON', 'ADV', 'VERB', 'NOUN', 'VERB', 'NOUN', 'DET', 'NOUN', 'AUX', 'NOUN', 'ADV', 'PRON', 'VERB', 'CONJ', 'PRON', 'VERB', 'DET', 'ADJ', 'NOUN', 'PRON', 'PRON', 'ADV', 'NOUN', 'P', 'DET', 'PART', 'NOUN', 'VERB', 'NOUN', 'P', 'DET', 'NOUN', 'PRON', 'VERB', 'P', 'PRON', 'ADV', 'VERB', 'ADJ', 'CONJ', 'NOUN', 'P', 'NOUN', 'PRON', 'VERB', 'NOUN', 'PRON', 'AUX', 'VERB', 'CONJ', 'ADJ', 'ADV', 'PRON', 'VERB', 'NOUN', 'CONJ', 'VERB', 'P', 'NOUN', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "natashatags = []\n",
    "for i in doc.tokens:\n",
    "    if i.pos != 'PUNCT':\n",
    "        if i.pos == 'ADP':\n",
    "            i.pos = i.pos.replace(i.pos,'P')\n",
    "        if i.pos == 'PROPN':\n",
    "            i.pos = i.pos.replace(i.pos,'NOUN')\n",
    "        if i.pos == 'CCONJ' or i.pos == 'SCONJ':\n",
    "            i.pos = i.pos.replace(i.pos,'CONJ')\n",
    "        #print(i.text,i.pos)\n",
    "        natashatags.append(i.pos)\n",
    "print(natashatags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hp3_XMf8RiWY"
   },
   "source": [
    "тут я вручную разметила теги, занесла их в словарь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "SH-UWVDDRiWZ"
   },
   "outputs": [],
   "source": [
    "mytags = ['NOUN','P','PRON','VERB','NOUN','VERB','P','NUM','NOUN','P','NOUN','P','ADJ','NOUN','VERB','NOUN','P','NOUN','PRON','VERB','ADJ','NOUN','P','NOUN','VERB','ADJ','NOUN','P','ADJ','ADJ','NOUN','P','P','ADJ','NOUN','ADV','VERB','NOUN','ADV','VERB','NOUN','NOUN','NUM','VERB','P','NOUN','NOUN','P','NOUN','NOUN','CONJ','VERB','ADV','CONJ','PRON','NOUN','CONJ','PRON','NOUN','NOUN','NOUN','P','NOUN','VERB','ADJ','ADJ','NOUN','PRON','ADV','VERB','NOUN','VERB','NOUN','PRON','NOUN','VERB','ADJ','ADV','PRON','VERB','CONJ','PRON','VERB','PRON','ADJ','NOUN','PRON','PRON','ADV','VERB','P','PRON','PART','NOUN','VERB','NOUN','P','PRON','NOUN','PRON','VERB','P','PRON','NOUN','VERB','NOUN','CONJ','VERB','P','NOUN','PRON','VERB','NOUN','PRON','VERB','ADJ','CONJ','ADJ','ADV','PRON','VERB','NOUN','CONJ','VERB','P','NOUN','NOUN']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "mxePeI0URiWf"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFIJR-peRiWk"
   },
   "source": [
    "посчитала accuracy, mystem круче всех справился с задачей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5YEW4boRiWl",
    "outputId": "c39ded3b-8e1d-47cf-d571-9c0a909496f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9449\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %.4f\" % accuracy_score(mystemtags, mytags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dBoe8T8CRiWq",
    "outputId": "e410548c-3f8c-439c-804f-b1ac9590603a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8346\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %.4f\" % accuracy_score(pymorphytags, mytags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FgnU5wz7RiWy",
    "outputId": "09759029-5072-4436-b80d-104c52aee06b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8898\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %.4f\" % accuracy_score(natashatags, mytags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "работа с текстом на английском языке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "rtV0GpdgzGx0"
   },
   "outputs": [],
   "source": [
    "text2 = 'The sun rose from behind the snowy mountains. The bay was painted pink and red by its rays. '+\\\n",
    "'А dog in the distance started to bay. A shepherd was herding sheep in a field of roses. '+\\\n",
    "'The boy was spooning his pumpkin soup with a wooden spoon. It was nice to play in the morning sun. '+\\\n",
    "'I enjoyed her enjoying the play.'+\\\n",
    "'The UN is an intergovernmental organization that aims to maintain international peace and security. '+\\\n",
    "'Do not judge what you do not know. Lasnik served as a superior court judge on the King County Superior Court from 1990 to 1998. '+\\\n",
    "'Man to Man is the third studio album by British soul band Hot Chocolate.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "сложности: слова-омонимы (rose глаг/rose сущ, bay глаг/bay сущ, spoon глаг/spoon сущ и тд), существительные, которые употребляются в кач-ве прил. (studio album - студийный альбом), числительное third записанное буквами, аббревиатура UN. Хотела еще взять слово через дефис (snow-covered но это было проблемно, тк эти слова разбивались на две части, а потом сравнивать с моим анализом сложно было бы...но случай проблемный все-таки)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk (весь последующий код был написан в колабе, тк в тетрадке юпитера модули не работают):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "zk0dlh1MRiW4"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import webtext\n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "lBYYDyk9fwD7",
    "outputId": "97977e1d-3d79-478e-8d0d-b0631c3b1815"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DET', 'NOUN', 'VERB', 'P', 'P', 'DET', 'NOUN', 'NOUN', 'DET', 'NOUN', 'VERB', 'VERB', 'NOUN', 'CONJ', 'ADJ', 'P', 'DET', 'NOUN', 'ADJ', 'NOUN', 'P', 'DET', 'NOUN', 'VERB', 'PRON', 'VERB', 'DET', 'NOUN', 'VERB', 'VERB', 'NOUN', 'P', 'DET', 'NOUN', 'P', 'NOUN', 'DET', 'NOUN', 'VERB', 'VERB', 'DET', 'NOUN', 'NOUN', 'P', 'DET', 'ADJ', 'NOUN', 'PRON', 'VERB', 'ADJ', 'PRON', 'VERB', 'P', 'DET', 'NOUN', 'NOUN', 'PRON', 'VERB', 'PRON', 'VERB', 'DET', 'NOUN', 'DET', 'NOUN', 'VERB', 'DET', 'ADJ', 'NOUN', 'PRON', 'VERB', 'PRON', 'VERB', 'ADJ', 'NOUN', 'CONJ', 'NOUN', 'NOUN', 'PART', 'VERB', 'PRON', 'PRON', 'VERB', 'PART', 'VERB', 'NOUN', 'VERB', 'P', 'DET', 'ADJ', 'NOUN', 'NOUN', 'P', 'DET', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'P', 'CD', 'PRON', 'CD', 'NOUN', 'PRON', 'NOUN', 'VERB', 'DET', 'ADJ', 'NOUN', 'NOUN', 'P', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "nltktags = []\n",
    "text2 = word_tokenize(text2)\n",
    "tags = nltk.pos_tag(text2)\n",
    "for x in tags:\n",
    "    tag = x[1]\n",
    "    if tag != '.':\n",
    "        if tag == 'NN' or tag == 'NNS' or tag == 'NNP':\n",
    "            tag = tag.replace(tag,'NOUN')   \n",
    "        if tag == 'PRP' or tag == 'WDT' or tag =='TO' or tag == 'WP':\n",
    "            tag = tag.replace(tag,'PRON')\n",
    "        if tag == 'DT' or tag == 'PRP$':\n",
    "            tag = tag.replace(tag,'DET') \n",
    "        if tag == 'VB' or tag == 'VBD' or tag == 'VBG' or tag == 'VBN' or tag == 'VBZ' or tag =='VBP':\n",
    "            tag = tag.replace(tag,'VERB') \n",
    "        if tag == 'IN':\n",
    "            tag = tag.replace(tag,'P')\n",
    "        if tag == 'JJ':\n",
    "            tag = tag.replace(tag,'ADJ')\n",
    "        if tag == 'CC':\n",
    "            tag = tag.replace(tag,'CONJ')\n",
    "        if tag == 'RB':\n",
    "            tag = tag.replace(tag,'PART')\n",
    "    nltktags.append(tag)\n",
    "print(nltktags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spacy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "Vt2M3HelRyD3",
    "outputId": "1cd17877-a41d-491f-8a44-144808561438"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DET', 'NOUN', 'VERB', 'P', 'P', 'DET', 'ADJ', 'NOUN', 'DET', 'NOUN', 'VERB', 'VERB', 'ADJ', 'CONJ', 'ADJ', 'P', 'DET', 'NOUN', 'DET', 'NOUN', 'P', 'DET', 'NOUN', 'VERB', 'PART', 'NOUN', 'DET', 'NOUN', 'VERB', 'VERB', 'NOUN', 'P', 'DET', 'NOUN', 'P', 'NOUN', 'DET', 'NOUN', 'VERB', 'VERB', 'DET', 'NOUN', 'NOUN', 'P', 'DET', 'ADJ', 'NOUN', 'PRON', 'VERB', 'ADJ', 'PART', 'VERB', 'P', 'DET', 'NOUN', 'NOUN', 'PRON', 'VERB', 'DET', 'VERB', 'DET', 'NOUN', 'DET', 'NOUN', 'VERB', 'DET', 'ADJ', 'NOUN', 'DET', 'VERB', 'PART', 'VERB', 'ADJ', 'NOUN', 'CONJ', 'NOUN', 'VERB', 'PART', 'VERB', 'PRON', 'PRON', 'VERB', 'PART', 'VERB', 'NOUN', 'VERB', 'SCONJ', 'DET', 'ADJ', 'NOUN', 'NOUN', 'P', 'DET', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'P', 'NUM', 'P', 'NUM', 'NOUN', 'P', 'NOUN', 'VERB', 'DET', 'ADJ', 'NOUN', 'NOUN', 'P', 'ADJ', 'NOUN', 'NOUN', 'NOUN', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "stags = []\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text2)\n",
    "for i, s in enumerate(doc.sents):\n",
    "    #print(\"\\n-- Sentence %d --\" % i)\n",
    "    for t in s:\n",
    "        tag = t.pos_\n",
    "        if tag == 'CCONJ':\n",
    "            tag = tag.replace(tag,'CONJ')\n",
    "        if tag == 'PROPN':\n",
    "            tag = tag.replace(tag,'NOUN')\n",
    "        if tag == 'AUX':\n",
    "            tag = tag.replace(tag,'VERB')\n",
    "        if tag == 'ADP':\n",
    "            tag = tag.replace(tag,'P')\n",
    "        if tag != 'PUNCT':\n",
    "        #print(t.text, tag, sep=\"\\t\")\n",
    "            stags.append(tag)\n",
    "print(stags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "By6ywBCQEoDn"
   },
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "G2Xht_DNFqXB",
    "outputId": "beab822b-960c-48c3-db2d-48781c4bdbff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DET', 'NOUN', 'VERB', 'P', 'P', 'DET', 'ADJ', 'NOUN', 'DET', 'NOUN', 'VERB', 'VERB', 'NOUN', 'CONJ', 'ADJ', 'P', 'DET', 'NOUN', 'NFP', 'NOUN', 'P', 'DET', 'NOUN', 'VERB', 'P', 'VERB', 'DET', 'NOUN', 'VERB', 'VERB', 'NOUN', 'P', 'DET', 'NOUN', 'P', 'NOUN', 'DET', 'NOUN', 'VERB', 'VERB', 'DET', 'NOUN', 'NOUN', 'P', 'DET', 'ADJ', 'NOUN', 'PRON', 'VERB', 'ADJ', 'P', 'VERB', 'P', 'DET', 'NOUN', 'NOUN', 'PRON', 'VERB', 'PRON', 'VERB', 'DET', 'NOUN', 'DET', 'NOUN', 'VERB', 'DET', 'ADJ', 'NOUN', 'PRON', 'VERB', 'P', 'VERB', 'ADJ', 'NOUN', 'CONJ', 'NOUN', 'VERB', 'PART', 'VERB', 'PRON', 'PRON', 'VERB', 'PART', 'VERB', 'NOUN', 'VERB', 'P', 'DET', 'ADJ', 'NOUN', 'NOUN', 'P', 'DET', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'P', 'NUM', 'P', 'NUM', 'NOUN', 'P', 'NOUN', 'VERB', 'DET', 'ADJ', 'NOUN', 'NOUN', 'P', 'ADJ', 'NOUN', 'NOUN', 'NOUN', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "flairtags = []\n",
    "sentence = Sentence(text2)\n",
    "tagger.predict(sentence)\n",
    "tagss = sentence.to_tagged_string()\n",
    "tags = re.findall('<\\w+>|<\\w+\\$>',tagss)\n",
    "for x in tags:\n",
    "    x = re.sub('<|>','',x)\n",
    "    if x == 'DT' or x == 'PRP$' or x == 'NPF':\n",
    "        x = x.replace(x,'DET')\n",
    "    if x == 'NN' or x == 'NNS' or x =='NNP':\n",
    "        x = x.replace(x,'NOUN')\n",
    "    if x == 'VBD' or x =='VBN' or x =='VB' or x =='VBG' or x =='VBZ' or x == 'VBP':\n",
    "        x = x.replace(x,'VERB')\n",
    "    if x == 'IN' or x == 'TO':\n",
    "        x = x.replace(x,'P')\n",
    "    if x == 'JJ':\n",
    "        x = x.replace(x,'ADJ')\n",
    "    if x == 'CC':\n",
    "        x = x.replace(x,'CONJ')\n",
    "    if x == 'PRP' or x == 'WDT' or x =='WP':\n",
    "        x = x.replace(x,'PRON')\n",
    "    if x == 'RB':\n",
    "        x = x.replace(x,'PART')\n",
    "    if x == 'CD':\n",
    "        x = x.replace(x,'NUM')\n",
    "    flairtags.append(x)\n",
    "print(flairtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "вручную размеченные теги:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "ZHmZ_E80eLde"
   },
   "outputs": [],
   "source": [
    "mytags2 = ['DET', 'NOUN', 'VERB', 'P','P', 'DET', 'ADJ', 'NOUN', 'DET', 'NOUN', 'VERB', 'ADJ', \n",
    "           'ADJ', 'CONJ', 'ADJ', 'P', 'DET', 'NOUN', 'DET', 'NOUN', 'P', 'DET', 'NOUN', \n",
    "           'VERB', 'P', 'VERB', 'DET', 'NOUN', 'VERB', 'VERB', 'NOUN', 'P','DET', 'NOUN', 'P',\n",
    "           'NOUN', 'DET', 'NOUN', 'VERB', 'VERB', 'DET', 'ADJ', 'NOUN', 'P', 'DET', 'ADJ', 'NOUN', \n",
    "           'PRON', 'VERB', 'ADJ', 'P', 'VERB', 'P', 'DET', 'ADJ', 'NOUN', 'PRON', 'VERB', 'DET',\n",
    "           'VERB', 'DET', 'NOUN', 'DET', 'NOUN', 'VERB', 'DET', 'ADJ', 'NOUN', 'PRON', 'VERB',\n",
    "           'P', 'VERB', 'ADJ', 'NOUN', 'CONJ', 'NOUN', 'VERB', 'PART', 'VERB', 'PRON', 'PRON',\n",
    "           'VERB', 'PART', 'VERB', 'NOUN', 'VERB', 'P', 'DET', 'ADJ', 'ADJ', 'NOUN', 'P', 'DET',\n",
    "           'NOUN', 'ADJ', 'ADJ', 'NOUN', 'P', 'NUM', 'P', 'NUM', 'NOUN', 'P', 'NOUN', 'VERB', 'DET', 'NUM', 'NOUN', \n",
    "           'NOUN', 'P', 'ADJ', 'ADJ', 'NOUN', 'ADJ', 'NOUN']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "лучше всех с задачей справился flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "k2_gwzWsc8g5",
    "outputId": "629a6813-e241-41ed-fd60-fe9135849c5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8087\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %.4f\" % accuracy_score(nltktags, mytags2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "HKhIbfe4dAzw",
    "outputId": "8db1f313-eed2-4ecf-d98e-60594b1a1561"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8696\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %.4f\" % accuracy_score(stags, mytags2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "CD9r-lcvdBAp",
    "outputId": "177ae014-dcf3-48ba-869f-bf883c5eac62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8957\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %.4f\" % accuracy_score(flairtags, mytags2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(дальше работа с синтаксисом и отзывами из прошлой домашки)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "есть предположение, что в отрицательных отзывах чаще используется частица \"не\" + прилаг. Я сгруппировала не+прил+сущ и проверила на положительном и на отрицательном отзыве, так ли это. Оказалось, что частицы вообще чаще в отриц используют"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bad = 'На самом деле нет, не самый любимый. Это отвратительное убожество, после которого непроизвольно прочистился желудок через горло. Это лютый фейл, просто ужасный, бездарный моток кинопленки!    Ээээх, давайте уж проанализируем, чем это творение так ужасно и почему у данного фильма такой коммерческий успех.    Фильм поставлен по роману (простите за слова «фильм» и «роман») Стефани Майер и является первой частью в плеяде богомерзких сиквелов. Но у этого фильма есть один плюс –он долее менее смотрибельный среди этих бессчётных сиквелов, но об этом позже.   Итак, сюжет. Сюжета как такового нет, а есть просто идея. Вот есть не каноничные смазливые вампиры, есть подкаченный оборотни и бревно, то есть Кристен Стюарт, то есть Белла Свон. И возня между ними, в которую поочерёдно включаются то какие-то придурканутые, театральные, педиковатые вампиры, то полиция. Фильм представляет собой нарезку кадров, каждый из которых представляет собой идею современного подросткового мира. Главная героиня –средняя, серая и не выразительная школьница из состоятельной семьи. Ее родители давно в разводе и это сделано тоже для впрыска реализма. Вон психологи считают 85% семей неблагополучными. Теперь ей придётся пожить у отца. Отец –суровый и необщительный шериф, у которого есть компания для осушения бочек пивасика в лице индейцев. И даже этого потенциально интересного персонажа сделали идиотом. А сделали для того, чтобы фильм не потерял своей концепции. Итак, возвращаемся к нашей героине. Она идет в школу, и зритель знакомится с чудесными и добрыми школьниками, которые тут же станут друзьями Беллы. Еще больше подростков со своими обывательскими проблемами и воображением. В школе она знакомиться с красивым, но не общительным парнем, а далее просто обычными словами не объяснишь. И все это вертятся в одном мире с испорченными флюидами и странными мечтами. И я ругаю этот фильм не за то, что он про подростков, а за то, что он отлучен от реальности. Такие шедевральные фильмы про подростков как Супер 8 или недавний сериал Очень странные дела были реальными и интересными. А здесь все представлено в виде мечт типичной девочки -школьницы: крутой парень с бабками, тачкой и прессом, томные затяжные диалоги, девчачий лайтовый музончик, интригующая тайна, размышления по ночам, реклама МакБука и треп с подружками. А все это прерывается пятисекундными очень дешево поставленных экшен сцен вроде нападения на рыбака или экстремальной авто прогулки на Астон Мартине V12. А потом все это заливается скучными бытовыми сценами типичного провинциального американца, еще больше рекламы (все для реализма, здесь это важно) и еще больше пустой болтологии ни о чём с короткими перерывами на около философские монологи главной героини, которые никак не привязаны к реальности. И все это подается медленно, тягуче, растянуто по максимуму. Каждую сцену на зло удлиняют и делает ее еще бессмысленнее. Все пять фильмов могли бы спокойно уместиться в два полуторачасовых лентах, а мы имеем более десяти часов!    Все, хорош! Теперь коротко об актерах. Кристен Стюарт здесь играет просто ужасно, но и ее героиня такая. К чему накидываться на актера если роль у него просто феерично тупая. Все остальные актеры имеют поменьше экранного времени, но крайней мере в этом фильме, и вообще не запомнились.    Спецэффектов здесь нет от слова совсем, а те, что и есть выглядят неестественно.    В итоге мы имеем двухчасовую нудную скучную нереалистичную мелодраму с няшными вампирами с блестками, которая дословно рисует на экране все типичные мечты глупых школьниц. Но эта шляпа собрала в прокате более 400 000 000$!!! И я уже молчу про успех первоисточника. Все школьницы и даже некоторые школьники хотят погрузиться в сладенький мир приятных любовных иллюзий, которых они не видят в реальности. Подобных книг как сумерки –миллионы и они уже выработали особое жанровое направление. Но даже не в содержании проблема. Фильм снят и подан плохо.    1 из 10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_good = 'Пальцы переплелись, а вместе с ними и чувства: моя боль, его боль...Один из самых удивительных феноменов в истории современного фэнтезийного кинематографа. Сколько существует на свете фильмов про вампиров? Тысячи. Сколько из них популярны настолько, насколько популярны Сумерки? Очень немногие, я бы даже сказала, что не знаю ни одного.    Единственный фильм из Саги, который я люблю необыкновенной, нежной любовью.   Я, наверное, не открою Америку, если скажу, что успех фильма зависит от режиссера. Разве может пустое и нудное Новолуние, совершенно бессмысленный Рассвет или весьма средненькое Затмение сравниться с этим? Для меня всегда будет существовать эта часть. Для меня Форкс будет дождливым городком в синеватых тонах, Белла - милой семнадцатилетней девушкой, человеком, а Джейкоб останется добрым и здоровым парнем, а не тем жестким и несчастным существом, которым впоследствии он станет.    Это высочайший уровень мастерства режиссуры. Кристен Стюарт действительно выглядит семнадцатилетней, на ее лице еще нет этого странного выражения заторможенности, которое появится в следующих частях. Эдвард Каллен в исполнении Роберта Паттинсона с каждой частью фильма выглядит все старше благодаря странному гриму, но именно здесь ему действительно всегда семнадцать. Форкс дождлив и таинственен, а некоторые сцены с чужими вампирами действительно внушают страх.    Весь фильм меня не покидало ощущение какого-то странного, мутного сна на грани яви. Бархатные звуки, гипнотизирующая музыка Картера Беруэлла (в следующих частях ее заменят на обычные песни, лишенные этого колдовского шарма).   Это один из тех фильмов, которые могут причинить столько радости и столько боли одновременно!   Я не смотрела ни одного фильма Кэтрин Хардвик кроме этого, но я всегда буду помнить о ее ответственности и мастерстве, и даже не буду ставить ее творение на одну полку с остальными частями халтурного продолжения сумеречной серии.    Вам может быть сколько угодно лет, вы можете быть сколько угодно интеллектуальны, умны, вы можете читать Кафку в оригинале но это не помешает вам любить этот фильм. Честно, не помешает.    Позвольте себе этот фильм. Позвольте себе увидеть таинственный городок, в котором происходят странные вещи, где лес таит опасность и полон чудес, где один неловкий взгляд может выдать вашу тайну. Почувствуйте жизнь, смерть и природу, которые слились в этом фильме в своей первозданной красоте.   10 из 10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ne_adj(some_text):\n",
    "    ne = ''\n",
    "    adj = ''\n",
    "    ana = m.analyze(some_text)\n",
    "    for word in ana:\n",
    "        if 'analysis' in word:\n",
    "            gr = word['analysis'][0]['gr']\n",
    "            pos = gr.split('=')[0].split(',')[0]\n",
    "            lex = word['text']\n",
    "            #print(lex,pos)\n",
    "            if ne == '':\n",
    "                if pos == 'PART':\n",
    "                    ne = lex\n",
    "            elif pos == 'A':\n",
    "                adj = lex\n",
    "            else:\n",
    "                if pos == 'S':\n",
    "                    print(ne, ' ', adj, ' ', lex)\n",
    "                    ne = ''\n",
    "                    adj = ''\n",
    "                elif pos != 'PART':\n",
    "                    ne = ''\n",
    "                else:\n",
    "                    ne = lex\n",
    "                \n",
    "                \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "угодно      лет\n"
     ]
    }
   ],
   "source": [
    "ne_adj(test_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Это   лютый   фейл\n",
      "это      творение\n",
      "просто      идея\n",
      "не   смазливые   вампиры\n",
      "И      возня\n",
      "не   выразительная   школьница\n",
      "Вон      психологи\n",
      "не   общительным   парнем\n"
     ]
    }
   ],
   "source": [
    "ne_adj(test_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "вторая группа это сущ + сущ (например: фильм отстой, актеры ужас), считаю, что такие штуки встречаются чаще в отрицательных отзывах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twonouns(some_text):\n",
    "    s = ''\n",
    "    ana = m.analyze(some_text)\n",
    "    for word in ana:\n",
    "        if 'analysis' in word:\n",
    "            gr = word['analysis'][0]['gr']\n",
    "            pos = gr.split('=')[0].split(',')[0]\n",
    "            lex = word['text']\n",
    "            if s == '':\n",
    "                if pos == 'S':\n",
    "                    s = lex\n",
    "            else:\n",
    "                if pos == 'S':\n",
    "                    print(s, ' ', lex)\n",
    "                    s = ''\n",
    "                elif pos != 'S':\n",
    "                    s = ''\n",
    "                else:\n",
    "                    s = lex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "свете   фильмов\n",
      "вампиров   Тысячи\n",
      "успех   фильма\n",
      "тонах   Белла\n",
      "девушкой   человеком\n",
      "уровень   мастерства\n",
      "режиссуры   Кристен\n",
      "выражения   заторможенности\n",
      "частях   Эдвард\n",
      "исполнении   Роберта\n",
      "частью   фильма\n",
      "грани   яви\n",
      "музыка   Картера\n",
      "фильма   Кэтрин\n",
      "жизнь   смерть\n"
     ]
    }
   ],
   "source": [
    "twonouns(test_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "моток   кинопленки\n",
      "успех   Фильм\n",
      "слова   фильм\n",
      "роман   Стефани\n",
      "сюжет   Сюжета\n",
      "Кристен   Стюарт\n",
      "Белла   Свон\n",
      "полиция   Фильм\n",
      "нарезку   кадров\n",
      "впрыска   реализма\n",
      "отца   Отец\n",
      "осушения   бочек\n",
      "лице   индейцев\n",
      "друзьями   Беллы\n",
      "виде   мечт\n",
      "девочки   школьницы\n",
      "бабками   тачкой\n",
      "тайна   размышления\n",
      "ночам   реклама\n",
      "авто   прогулки\n",
      "Астон   Мартине\n",
      "актерах   Кристен\n",
      "успех   первоисточника\n",
      "сумерки   миллионы\n",
      "содержании   проблема\n"
     ]
    }
   ],
   "source": [
    "twonouns(test_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "третья группа это соответственно прил + сущ (чаще отрицательные отзывы более гневные и выразительные с точки зрения языка, так что такие штуки чаще в отрицательных отзывах) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_noun(some_text):\n",
    "    adj = ''\n",
    "    ana = m.analyze(some_text)\n",
    "    for word in ana:\n",
    "        if 'analysis' in word:\n",
    "            gr = word['analysis'][0]['gr']\n",
    "            pos = gr.split('=')[0].split(',')[0]\n",
    "            lex = word['text']\n",
    "            #print(lex,pos)\n",
    "            if adj == '':\n",
    "                if pos == 'A':\n",
    "                    adj = lex\n",
    "            else:\n",
    "                if pos == 'S':\n",
    "                    print(adj, ' ', lex)\n",
    "                    adj = ''\n",
    "                elif pos != 'A':\n",
    "                    adj = ''\n",
    "                else:\n",
    "                    adj = lex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "удивительных   феноменов\n",
      "фэнтезийного   кинематографа\n",
      "популярны   Сумерки\n",
      "Единственный   фильм\n",
      "нежной   любовью\n",
      "нудное   Новолуние\n",
      "бессмысленный   Рассвет\n",
      "средненькое   Затмение\n",
      "дождливым   городком\n",
      "синеватых   тонах\n",
      "семнадцатилетней   девушкой\n",
      "здоровым   парнем\n",
      "несчастным   существом\n",
      "высочайший   уровень\n",
      "странного   выражения\n",
      "следующих   частях\n",
      "странному   гриму\n",
      "чужими   вампирами\n",
      "мутного   сна\n",
      "Бархатные   звуки\n",
      "следующих   частях\n",
      "обычные   песни\n",
      "колдовского   шарма\n",
      "халтурного   продолжения\n",
      "сумеречной   серии\n",
      "таинственный   городок\n",
      "странные   вещи\n",
      "полон   чудес\n",
      "неловкий   взгляд\n",
      "первозданной   красоте\n"
     ]
    }
   ],
   "source": [
    "adj_noun(test_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "отвратительное   убожество\n",
      "лютый   фейл\n",
      "бездарный   моток\n",
      "данного   фильма\n",
      "коммерческий   успех\n",
      "богомерзких   сиквелов\n",
      "бессчётных   сиквелов\n",
      "смазливые   вампиры\n",
      "педиковатые   вампиры\n",
      "подросткового   мира\n",
      "Главная   героиня\n",
      "выразительная   школьница\n",
      "состоятельной   семьи\n",
      "необщительный   шериф\n",
      "интересного   персонажа\n",
      "добрыми   школьниками\n",
      "обывательскими   проблемами\n",
      "общительным   парнем\n",
      "обычными   словами\n",
      "испорченными   флюидами\n",
      "странными   мечтами\n",
      "шедевральные   фильмы\n",
      "недавний   сериал\n",
      "странные   дела\n",
      "типичной   девочки\n",
      "крутой   парень\n",
      "затяжные   диалоги\n",
      "лайтовый   музончик\n",
      "интригующая   тайна\n",
      "экшен   сцен\n",
      "экстремальной   авто\n",
      "бытовыми   сценами\n",
      "провинциального   американца\n",
      "пустой   болтологии\n",
      "короткими   перерывами\n",
      "философские   монологи\n",
      "главной   героини\n",
      "полуторачасовых   лентах\n",
      "экранного   времени\n",
      "крайней   мере\n",
      "нереалистичную   мелодраму\n",
      "няшными   вампирами\n",
      "типичные   мечты\n",
      "глупых   школьниц\n",
      "сладенький   мир\n",
      "любовных   иллюзий\n",
      "Подобных   книг\n",
      "жанровое   направление\n"
     ]
    }
   ],
   "source": [
    "adj_noun(test_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "все эти функции я вставила в код из первой домашки, вышло честно говоря не очень, accuracy стала еще меньше (посмотреть код можно в файле nlp1_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "nlp2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
